{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Dropout, Reshape, Permute, GRU, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembacaan data training\n",
    "\n",
    "train_df = pd.read_csv('[Dataset]_Train_(Perawatan-Pesawat).csv', sep=\",\")\n",
    "train_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pembacaan ground truth data \n",
    "test_df = pd.read_csv('[Dataset]_Test_(Perawatan-Pesawat).csv', sep=\",\")\n",
    "test_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pembacaan data test\n",
    "truth_df = pd.read_csv('truth.txt', sep=\" \")\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data labelling dilakukan agar data menjadi sesuai dengan format algoritma, sebenarnya dari sebelum adanya labelling, data ini sudah terlabeli dengan label_bnc, label_mcc dan ttf (sebagai RUL). langkah ini hanya digunakan untuk memastikan saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data labelling\n",
    "\n",
    "ttf = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "ttf.columns = ['id', 'max']\n",
    "train_df = train_df.merge(ttf, on=['id'], how='left')\n",
    "train_df['ttf'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi data train\n",
    "\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','ttf','label_bnc','label_mcc'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisasi data test\n",
    "\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generasi kolom pada data testing\n",
    "\n",
    "ttf = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "ttf.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = ttf['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate ttf for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['ttf'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data\n",
    "test_df['label_bnc'] = np.where(test_df['ttf'] <= w1, 1, 0 )\n",
    "test_df['label_mcc'] = test_df['label_bnc']\n",
    "test_df.loc[test_df['ttf'] <= w0, 'label_mcc'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# TODO for debug \n",
    "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
    "val=list(gen_sequence(train_df[train_df['id']==1], sequence_length, sequence_cols))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    return data_matrix[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['ttf']) \n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape\n",
    "\n",
    "##################################\n",
    "# Modeling\n",
    "##################################\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=2, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for R^2\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['r2_keras'])\n",
    "plt.plot(history.history['val_r2_keras'])\n",
    "plt.title('model r^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"/Output/model_r2.png\")\n",
    "\n",
    "# summarize history for MAE\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"/Output/model_mae.png\")\n",
    "\n",
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# fig_acc.savefig(\"./Output/model_regression_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('\\nMAE: {}'.format(scores[1]))\n",
    "print('\\nR^2: {}'.format(scores[2]))\n",
    "\n",
    "y_pred = model.predict(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "\n",
    "test_set = pd.DataFrame(y_pred)\n",
    "# test_set.to_csv('/Output/submit_train.csv', index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################\n",
    "# EVALUATE ON TEST DATA\n",
    "##################################\n",
    "\n",
    "# We pick the last sequence for each id in the test data\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "print(\"seq_array_test_last\")\n",
    "#print(seq_array_test_last)\n",
    "print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "#print(\"y_mask\")\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "label_array_test_last = test_df.groupby('id')['ttf'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "# label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1)\n",
    "print(label_array_test_last.shape)\n",
    "print(\"label_array_test_last\")\n",
    "print(label_array_test_last)\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "    # test metrics\n",
    "    # label_array_test_last = joblib.load(label_array_test_last)\n",
    "    model.predict(label_array_test_last)\n",
    "    type(label_array_test_last)\n",
    "    # type(seq_array_test_last)\n",
    "    # label_array_test_last = label_array_test_last.predict(label_array_test_last)\n",
    "    # seq_array_test_last = seq_array_test_last.reshape(seq_array_test_last.shape[0],1).astype(np.float32)\n",
    "    seq_array_test_last = seq_array_test_last.predict(seq_array_test_last)\n",
    "    scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "    # scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last)\n",
    "    print('\\nMAE: {}'.format(scores_test[1]))\n",
    "    print('\\nR^2: {}'.format(scores_test[2]))\n",
    "\n",
    "    y_pred_test = estimator.predict(seq_array_test_last)\n",
    "    y_true_test = label_array_test_last\n",
    "\n",
    "    test_set = pd.DataFrame(y_pred_test)\n",
    "    test_set.to_csv('/Output/submit_test.csv', index = None)\n",
    "\n",
    "    # Plot in blue color the predicted data and in green color the\n",
    "    # actual data to verify visually the accuracy of the model.\n",
    "    fig_verify = plt.figure(figsize=(100, 50))\n",
    "    plt.plot(y_pred_test, color=\"blue\")\n",
    "    plt.plot(y_true_test, color=\"green\")\n",
    "    plt.title('prediction')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('row')\n",
    "    plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "    plt.show()\n",
    "    # fig_verify.savefig(\"/Output/model_regression_verify.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
