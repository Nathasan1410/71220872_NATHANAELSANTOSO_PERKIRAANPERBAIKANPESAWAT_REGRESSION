{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70-XsHkDGUKu",
        "outputId": "9e543864-b253-4cd7-f5ea-8efb71833f73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 6337672154193076403\n",
              " xla_global_id: -1]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZkz29ilGWd9",
        "outputId": "b43f76e4-88ba-408d-f022-5623dd48e16e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cat' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/meminfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6UbfwSrGZsH",
        "outputId": "f08e9b19-857b-4e20-e09b-8b0800349fe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cat' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bFU-T5cNeEe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://raw.githubusercontent.com/umbertogriffo/Predictive-Maintenance-using-LSTM/master/Dataset/PM_test.txt -O PM_test.txt\n",
        "!wget -q https://raw.githubusercontent.com/umbertogriffo/Predictive-Maintenance-using-LSTM/master/Dataset/PM_train.txt -O PM_train.txt\n",
        "!wget -q https://raw.githubusercontent.com/umbertogriffo/Predictive-Maintenance-using-LSTM/master/Dataset/PM_truth.txt -O PM_truth.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzoL9ofEgJJf",
        "outputId": "ec4cf0ad-990a-4b15-9dee-b9f6a2b2dd3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BsYPkY7gYGx"
      },
      "source": [
        "# Binary classification\n",
        "Predict if an asset will fail within certain time frame (e.g. cycles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QfpzPSgG-If3"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "\n",
        "# define path to save model\n",
        "model_path = 'binary_model.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EilFg--x-ety"
      },
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JjbfnUZGgc3C"
      },
      "outputs": [],
      "source": [
        "# read training data - It is the aircraft engine run-to-failure data.\n",
        "train_df = pd.read_csv('[Dataset]_Train_(Perawatan-Pesawat).csv', sep=\",\")\n",
        "# train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "# train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "#                      's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "#                      's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])\n",
        "\n",
        "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
        "test_df = pd.read_csv('[Dataset]_test_(Perawatan-Pesawat).csv', sep=\",\")\n",
        "# test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "# test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "#                      's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "#                      's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('truth.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEpD7amS-lpu"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ulY14O06knOI"
      },
      "outputs": [],
      "source": [
        "#######\n",
        "# TRAIN\n",
        "#######\n",
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification,\n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
        "\n",
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
        "                             columns=cols_normalize,\n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)\n",
        "\n",
        "######\n",
        "# TEST\n",
        "######\n",
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
        "                            columns=cols_normalize,\n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "truth_df.drop('more', axis=1, inplace=True)\n",
        "\n",
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)\n",
        "\n",
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57FSFDb4-r3d"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSInZu-EkFtf",
        "outputId": "bfbda636-4860-4df6-9ac4-27666a869eb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m50,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,651</span> (315.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,651\u001b[0m (315.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,651</span> (315.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,651\u001b[0m (315.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=binary_model.h5",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 90\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[0;32m     87\u001b[0m \u001b[39m# fit the network\u001b[39;00m\n\u001b[0;32m     88\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(seq_array, label_array, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, validation_split\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     89\u001b[0m           callbacks \u001b[39m=\u001b[39m [keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m---> 90\u001b[0m                        keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mModelCheckpoint(model_path,monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m, save_best_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)]\n\u001b[0;32m     91\u001b[0m           )\n\u001b[0;32m     93\u001b[0m \u001b[39m# list all data in history\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39mprint\u001b[39m(history\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mkeys())\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.keras\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe filepath provided must end in `.keras` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(Keras model format). Received: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfilepath=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n",
            "\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=binary_model.h5"
          ]
        }
      ],
      "source": [
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 50\n",
        "\n",
        "# function to reshape features into (samples, time steps, features)\n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    # for one id I put all the rows in a single matrix\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    # Iterate over two lists in parallel.\n",
        "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
        "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
        "    # 0 50 -> from row 0 to row 50\n",
        "    # 1 51 -> from row 1 to row 51\n",
        "    # 2 52 -> from row 2 to row 52\n",
        "    # ...\n",
        "    # 111 191 -> from row 111 to 191\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]\n",
        "\n",
        "# pick the feature columns\n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)\n",
        "\n",
        "# generator for the sequences\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols))\n",
        "           for id in train_df['id'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "seq_array.shape\n",
        "\n",
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    # For one id I put all the labels in a single matrix.\n",
        "    # For example:\n",
        "    # [[1]\n",
        "    # [4]\n",
        "    # [1]\n",
        "    # [5]\n",
        "    # [9]\n",
        "    # ...\n",
        "    # [200]]\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    # I have to remove the first seq_length labels\n",
        "    # because for one id the first sequence of seq_length size have as target\n",
        "    # the last label (the previus ones are discarded).\n",
        "    # All the next id's sequences will have associated step by step one label as target.\n",
        "    return data_matrix[seq_length:num_elements, :]\n",
        "\n",
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1'])\n",
        "             for id in train_df['id'].unique()]\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape\n",
        "\n",
        "# Next, we build a deep network.\n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.\n",
        "# Dropout is also applied after each LSTM layer to control overfitting.\n",
        "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
        "# build the network\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(\n",
        "         input_shape=(sequence_length, nb_features),\n",
        "         units=100,\n",
        "         return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(\n",
        "          units=50,\n",
        "          return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# fit the network\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "          )\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EzokY6U0Ghbl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWyZJ2mP-1pB"
      },
      "source": [
        "## Model Evaluation on Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FpVYSzXkmk5l",
        "outputId": "7479ee83-dab1-450d-b1c2-57f5bd94292a"
      },
      "outputs": [],
      "source": [
        "# summarize history for Accuracy\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_accuracy.png\")\n",
        "\n",
        "# summarize history for Loss\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_loss.png\")\n",
        "\n",
        "# training metrics\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('Accurracy: {}'.format(scores[1]))\n",
        "\n",
        "# make predictions and compute confusion matrix\n",
        "y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
        "y_true = label_array\n",
        "\n",
        "test_set = pd.DataFrame(y_pred)\n",
        "test_set.to_csv('binary_submit_train.csv', index = None)\n",
        "\n",
        "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# compute precision and recall\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print( 'precision = ', precision, '\\n', 'recall = ', recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxvEuR4S-6VI"
      },
      "source": [
        "## Model Evaluation on Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yT96j4rkL0K"
      },
      "outputs": [],
      "source": [
        "# We pick the last sequence for each id in the test data\n",
        "\n",
        "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:]\n",
        "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
        "#print(\"seq_array_test_last\")\n",
        "#print(seq_array_test_last)\n",
        "#print(seq_array_test_last.shape)\n",
        "\n",
        "# Similarly, we pick the labels\n",
        "\n",
        "#print(\"y_mask\")\n",
        "# serve per prendere solo le label delle sequenze che sono almeno lunghe 50\n",
        "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
        "#print(\"y_mask\")\n",
        "#print(y_mask)\n",
        "label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
        "#print(label_array_test_last.shape)\n",
        "#print(\"label_array_test_last\")\n",
        "#print(label_array_test_last)\n",
        "\n",
        "# if best iteration's model was saved then load and use it\n",
        "if os.path.isfile(model_path):\n",
        "    estimator = load_model(model_path)\n",
        "\n",
        "# test metrics\n",
        "scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
        "print('Accurracy: {}'.format(scores_test[1]))\n",
        "\n",
        "# make predictions and compute confusion matrix\n",
        "y_pred_test = estimator.predict_classes(seq_array_test_last)\n",
        "y_true_test = label_array_test_last\n",
        "\n",
        "test_set = pd.DataFrame(y_pred_test)\n",
        "test_set.to_csv('binary_submit_test.csv', index = None)\n",
        "\n",
        "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
        "cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "print(cm)\n",
        "\n",
        "# compute precision and recall\n",
        "precision_test = precision_score(y_true_test, y_pred_test)\n",
        "recall_test = recall_score(y_true_test, y_pred_test)\n",
        "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
        "print( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )\n",
        "\n",
        "# Plot in blue color the predicted data and in green color the\n",
        "# actual data to verify visually the accuracy of the model.\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test, color=\"blue\")\n",
        "plt.plot(y_true_test, color=\"green\")\n",
        "plt.title('prediction')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('row')\n",
        "plt.legend(['predicted', 'actual data'], loc='upper left')\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_verify.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1T8y5qA91x8"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "- [1] Deep Learning for Predictive Maintenance https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb\n",
        "- [2] Predictive Maintenance: Step 2A of 3, train and evaluate regression models https://gallery.cortanaintelligence.com/Experiment/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2\n",
        "- [3] A. Saxena and K. Goebel (2008). \"Turbofan Engine Degradation Simulation Data Set\", NASA Ames Prognostics Data Repository (https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan), NASA Ames Research Center, Moffett Field, CA\n",
        "- [4] Understanding LSTM Networks http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4BsYPkY7gYGx",
        "ElhakcHtnX9J"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
